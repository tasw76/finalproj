[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "Download, bring in the data, preview, and check for missing values\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the data\ndata1 &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data1)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n# check for missing values\ncolSums(is.na(data1))   # No missing noted\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\ndata1\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;"
  },
  {
    "objectID": "EDA.html#load-the-data",
    "href": "EDA.html#load-the-data",
    "title": "EDA",
    "section": "",
    "text": "Download, bring in the data, preview, and check for missing values\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Load the data\ndata1 &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(data1)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n# check for missing values\ncolSums(is.na(data1))   # No missing noted\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\ndata1\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1               0      1        1         1    40      1      0\n 2               0      0        0         0    25      1      0\n 3               0      1        1         1    28      0      0\n 4               0      1        0         1    27      0      0\n 5               0      1        1         1    24      0      0\n 6               0      1        1         1    25      1      0\n 7               0      1        0         1    30      1      0\n 8               0      1        1         1    25      1      0\n 9               1      1        1         1    30      1      0\n10               0      0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;"
  },
  {
    "objectID": "EDA.html#preprocess-variables",
    "href": "EDA.html#preprocess-variables",
    "title": "EDA",
    "section": "Preprocess variables",
    "text": "Preprocess variables\n\nIn this part, I convert several variables to factor variables with meaningful level names\n\n\n# convert binary numeric variables to factors\ndata1 &lt;- data1 %&gt;%\n  mutate(\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\"))\n  )\n\ndata1\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n             &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1               0 Yes    Yes              1    40 Yes    No    \n 2               0 No     No               0    25 Yes    No    \n 3               0 Yes    Yes              1    28 No     No    \n 4               0 Yes    No               1    27 No     No    \n 5               0 Yes    Yes              1    24 No     No    \n 6               0 Yes    Yes              1    25 Yes    No    \n 7               0 Yes    No               1    30 Yes    No    \n 8               0 Yes    Yes              1    25 Yes    No    \n 9               1 Yes    Yes              1    30 Yes    No    \n10               0 No     No               1    24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\nglimpse(data1)\n\nRows: 253,680\nColumns: 22\n$ Diabetes_binary      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0…\n$ HighBP               &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, N…\n$ HighChol             &lt;fct&gt; Yes, No, Yes, No, Yes, Yes, No, Yes, Yes, No, No,…\n$ CholCheck            &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;fct&gt; Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, No, Yes…\n$ Stroke               &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ HeartDiseaseorAttack &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, No, No, …\n$ PhysActivity         &lt;fct&gt; No, Yes, No, Yes, Yes, Yes, No, Yes, No, No, Yes,…\n$ Fruits               &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, No, No, Yes, No, Yes,…\n$ Veggies              &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ HvyAlcoholConsump    &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ AnyHealthcare        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ NoDocbcCost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ GenHlth              &lt;fct&gt; poor, good, poor, very_good, very_good, very_good…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n$ Sex                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Age                  &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "Summarizations",
    "text": "Summarizations\n\nSummary of the response variable (Diabetes_binary)\nIt shows there are about 14% of the total population in the data have diabetes\n\n\nsummary(data1)\n\n Diabetes_binary  HighBP       HighChol       CholCheck           BMI       \n Min.   :0.0000   No :144851   No :146089   Min.   :0.0000   Min.   :12.00  \n 1st Qu.:0.0000   Yes:108829   Yes:107591   1st Qu.:1.0000   1st Qu.:24.00  \n Median :0.0000                             Median :1.0000   Median :27.00  \n Mean   :0.1393                             Mean   :0.9627   Mean   :28.38  \n 3rd Qu.:0.0000                             3rd Qu.:1.0000   3rd Qu.:31.00  \n Max.   :1.0000                             Max.   :1.0000   Max.   :98.00  \n Smoker       Stroke       HeartDiseaseorAttack PhysActivity Fruits      \n No :141257   No :243388   No :229787           No : 61760   No : 92782  \n Yes:112423   Yes: 10292   Yes: 23893           Yes:191920   Yes:160898  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n    Veggies       HvyAlcoholConsump AnyHealthcare     NoDocbcCost     \n Min.   :0.0000   No :239424        Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:1.0000   Yes: 14256        1st Qu.:1.0000   1st Qu.:0.00000  \n Median :1.0000                     Median :1.0000   Median :0.00000  \n Mean   :0.8114                     Mean   :0.9511   Mean   :0.08418  \n 3rd Qu.:1.0000                     3rd Qu.:1.0000   3rd Qu.:0.00000  \n Max.   :1.0000                     Max.   :1.0000   Max.   :1.00000  \n      GenHlth         MentHlth         PhysHlth         DiffWalk     \n excellent:45299   Min.   : 0.000   Min.   : 0.000   Min.   :0.0000  \n very_good:89084   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.:0.0000  \n good     :75646   Median : 0.000   Median : 0.000   Median :0.0000  \n fair     :31570   Mean   : 3.185   Mean   : 4.242   Mean   :0.1682  \n poor     :12081   3rd Qu.: 2.000   3rd Qu.: 3.000   3rd Qu.:0.0000  \n                   Max.   :30.000   Max.   :30.000   Max.   :1.0000  \n      Sex              Age           Education        Income     \n Min.   :0.0000   Min.   : 1.000   Min.   :1.00   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.: 6.000   1st Qu.:4.00   1st Qu.:5.000  \n Median :0.0000   Median : 8.000   Median :5.00   Median :7.000  \n Mean   :0.4403   Mean   : 8.032   Mean   :5.05   Mean   :6.054  \n 3rd Qu.:1.0000   3rd Qu.:10.000   3rd Qu.:6.00   3rd Qu.:8.000  \n Max.   :1.0000   Max.   :13.000   Max.   :6.00   Max.   :8.000  \n\n# summary of response variable\nresponse_summary &lt;- data1 %&gt;%\n  count(Diabetes_binary) %&gt;%\n  mutate(Percentage = round(n / sum(n) * 100, 2))\nprint(response_summary)\n\n# A tibble: 2 × 3\n  Diabetes_binary      n Percentage\n            &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1               0 218334       86.1\n2               1  35346       13.9\n\n\n\nSummary of a predictor ‘GenHlth’\nThe results show the counts and percentages details in each of the five levels of GenHlth: more than 80% of the population is in good, very good, or excellent health condition\n\n\ngenhlth_summary &lt;- data1 %&gt;%\n  count(GenHlth) %&gt;%\n  mutate(Percentage = round(n / sum(n) * 100, 2))\n\nprint(genhlth_summary)\n\n# A tibble: 5 × 3\n  GenHlth       n Percentage\n  &lt;fct&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 excellent 45299      17.9 \n2 very_good 89084      35.1 \n3 good      75646      29.8 \n4 fair      31570      12.4 \n5 poor      12081       4.76\n\n\n\nSummary of a numeric predictor BMI\nResults show mean, median, standard deviation, min and max of BMI\n\n\nbmi_summary &lt;- data1 %&gt;%\n  summarise(\n    Mean = round(mean(BMI, na.rm = TRUE), 2),\n    Median = round(median(BMI, na.rm = TRUE), 2),\n    SD = round(sd(BMI, na.rm = TRUE), 2),\n    Min = round(min(BMI, na.rm = TRUE), 2),\n    Max = round(max(BMI, na.rm = TRUE), 2)\n  )\nprint(bmi_summary)\n\n# A tibble: 1 × 5\n   Mean Median    SD   Min   Max\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  28.4     27  6.61    12    98\n\n\n\nproduce a correlation between Diabetes_binary and BMI\nthe resulting correlation shows that they are not significantly correlated - there is a weak negative correlation (-0.2168) between the predictor and the response. (correlation between Diabetes_binary and other predictors are produced but not illustrated here)\n\n\nlibrary(ltm)\n\nWarning: package 'ltm' was built under R version 4.3.3\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: msm\n\n\nWarning: package 'msm' was built under R version 4.3.3\n\n\nLoading required package: polycor\n\n\nWarning: package 'polycor' was built under R version 4.3.3\n\ndata1$Diabetes_binary &lt;- as.numeric(as.character(data1$Diabetes_binary))\ncorrelation &lt;- biserial.cor(data1$BMI, data1$Diabetes_binary)\nprint(correlation)\n\n[1] -0.2168431\n\n\n\nProduce a stacked bar plot to visualize the relationship between Diabetes_binary and HighBP\nBased on the plot, it seems that the likelihood of having diabetes is significantly higher if the individual has high blood pressure (other predictors are also plotted but not illustrated here)\n\n\nlibrary(ggplot2)\n\n# Convert the response to factors \ndata1 &lt;- data1 %&gt;%\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"No\", \"Yes\")))\n\n# Create a stacked bar plot for HighBP vs Diabetes_binary\nggplot(data1, aes(x = HighBP, fill = Diabetes_binary)) +\n  geom_bar(position = \"fill\") +\n  labs(\n    x = \"High Blood Pressure\",\n    y = \"Proportion\",\n    fill = \"Diabetes Status\",\n    title = \"Relationship Between High Blood Pressure and Diabetes\"\n  ) +\n  scale_y_continuous(labels = scales::percent) +\n  theme_minimal() \n\n\n\n\n\ndata1\n\n# A tibble: 253,680 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No              Yes    Yes              1    40 Yes    No    \n 2 No              No     No               0    25 Yes    No    \n 3 No              Yes    Yes              1    28 No     No    \n 4 No              Yes    No               1    27 No     No    \n 5 No              Yes    Yes              1    24 No     No    \n 6 No              Yes    Yes              1    25 Yes    No    \n 7 No              Yes    No               1    30 Yes    No    \n 8 No              Yes    Yes              1    25 Yes    No    \n 9 Yes             Yes    Yes              1    30 Yes    No    \n10 No              No     No               1    24 No     No    \n# ℹ 253,670 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "In this part, the goal is to create models for predicting the Diabetes_binary variable\nUse log-loss as the metric the evaluate the models\nFor both classification tree and random forest model type, use log-loss with 5 fold cross-validation to select the best model, respectively.\n\n\n\n\nA classification tree is a type of decision tree algorithm used for categorical response variables (in this example, Diabetes_binary)\nThe tree starts with a root node and then selects features that best splits the data into classes. At each node there will be a split, creating branches. Splitting stops when a criterion is met. Then at the leaf nodes, the class label is assigned based on the majority class of the training data in that node. To classify a new data point, the tree starts at the root node and follows the branches based on the feature values of the data point until it reaches a leaf node.\nNodes represent features; branches represent decision rules; leaves represent class labels (predicted outcomes)\nSplit the data as 70% being training set and 30% being test set\n\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(rsample)\nlibrary(dplyr)\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\nlibrary(tibble)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata1 &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata1 &lt;- data1 %&gt;%\n  mutate(\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\"))\n  )\ndata1 &lt;- data1 %&gt;%\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"No\", \"Yes\")))\n# Set a seed for reproducibility\nset.seed(123)\n\n# Split the data into training (70%) and test sets (30%)\n# resulting training set size: 177,575; resulting test set size: 76105\n\n# data_split &lt;- initial_split(data %&gt;% mutate(Diabetes_binary = factor(Diabetes_binary)), \n#                             prop = 0.7, \n#                             strata = Diabetes_binary)\n\n# # not working when rendering\n# data_split &lt;- initial_split(\n#   data = data %&gt;% dplyr::mutate(Diabetes_binary = as.factor(Diabetes_binary)), \n#   prop = 0.7, \n#   strata = Diabetes_binary\n# )\n\n# Get row indices for training set\n# train_indices &lt;- sample(seq_len(253680), size = floor(0.7 * 253680))\n# my_data &lt;- data \n# # Split the data\n# \n# # training_data &lt;- slice(data, train_indices)\n# # test_data &lt;- slice(data, -train_indices)\n# \n# training_data &lt;- my_data[train_indices, ]  # 70% of the data\n# test_data &lt;- my_data[-train_indices, ]        # Remaining 30% of the data\n# test_data\n\n\n# data_split &lt;- initial_split(data1, prop = 0.7, strata = Diabetes_binary)\n# training_data &lt;- training(data_split)\n# test_data &lt;- testing(data_split)\n# print(class(data1))\n\ntotal_rows &lt;- 253680\ntrain_indices &lt;- sample(seq_len(total_rows), size = floor(0.7 * total_rows))\n\n# Split using `data1`\ntraining_data &lt;- data1[train_indices, ]\ntest_data &lt;- data1[-train_indices, ]\ntest_data\n\n# A tibble: 76,104 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No              No     No               0    25 Yes    No    \n 2 No              Yes    No               1    27 No     No    \n 3 No              Yes    Yes              1    24 No     No    \n 4 No              Yes    Yes              1    25 Yes    No    \n 5 Yes             Yes    Yes              1    28 No     No    \n 6 No              No     Yes              1    33 Yes    Yes   \n 7 No              Yes    Yes              1    28 Yes    No    \n 8 No              No     No               1    32 No     No    \n 9 Yes             Yes    Yes              1    37 Yes    Yes   \n10 Yes             Yes    Yes              1    28 Yes    No    \n# ℹ 76,094 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n# finally it worked. In Modeling.qmd, I need to do every step that I did in EDA.qmd. Environment of running quarto render is different from that in local PC \n\n\ncheck the sizes of training and test sets\n\n\ncat(\"Training set size:\", nrow(training_data), \"\\n\")\ncat(\"Test set size:\", nrow(test_data), \"\\n\")\n\n\nFit a classification tree with varying values for the complexity parameter\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\ntraining_data &lt;- training_data %&gt;%\n  mutate(Diabetes_binary = as.factor(Diabetes_binary))\n\n# Set seed for reproducibility\nset.seed(123)\n# Choose a smaller dataset without compromising the results (otherwise the execution time is too long)\nsmall_training_data &lt;- training_data %&gt;%\n  slice_sample(prop = 0.4)\n\n# Specify the classification tree model with 6 predictor variables:  HighBP + HighChol + BMI + PhysActivity + HeartDiseaseorAttack + Age\ntree_recipe &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + GenHlth + Age, data = small_training_data)\n\n# Define the classification tree model with tuning parameters\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),  # Parameter to tune\n  tree_depth = tune(),       # Parameter to tune\n  min_n = tune()             # Parameter to tune\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"rpart\")\n\n\n# Create a workflow\ntree_workflow &lt;- workflow() %&gt;%\n  add_recipe(tree_recipe) %&gt;%\n  add_model(tree_spec)\n\n\n# Create 5-fold cross-validation folds on the small dataset\ncv_folds &lt;- vfold_cv(small_training_data, v = 5, strata = Diabetes_binary)\n\n# Set up a grid of hyperparameters to tune\ngrid &lt;- grid_regular(\n  cost_complexity(),\n  tree_depth(),\n  min_n(),\n  levels = 3\n)\n\n# Tune the model using log-loss. This step takes long time to execute\nset.seed(123)\n\ntree_results &lt;- tune_grid(\n  tree_workflow,\n  resamples = cv_folds,\n  grid = grid,\n  metrics = metric_set(mn_log_loss)  # Use log-loss as the evaluation metric\n)\n\n# Select the best based on log-loss\nbest_params &lt;- select_best(tree_results, metric = \"mn_log_loss\")\n\n# Finalize the workflow with the best parameters\nfinal_tree &lt;- finalize_workflow(tree_workflow, best_params)\n\n# Train the final model on the training set\nfinal_tree_fit &lt;- fit(final_tree, data = small_training_data)\n\n# Display the results\nautoplot(tree_results) + labs(title = \"Log-Loss for Classification Tree\")\n\n\n\n\n\n# Print the best parameters\nprint(best_params)\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         15    40 Preprocessor1_Model25\n\n\n\n# check the resulting model fit\ntree_results\nfinal_tree\nfinal_tree_fit\n\n\n\n\n\nA random forest is an ensemble machine learning algorithm that builds and combines multiple decision trees to improve accuracy and robustness.\nIt Creates many decision trees (often hundreds or thousands) from random subsets of the data (bootstrapping). At each split in a tree, only a random subset of predictors is considered. In our case, the previous step uses a classification tree model. And in this step, random forest uses majority voting where the class predicted most often by the trees is chosen.\nRandom forest improves over a Basic classification tree. It reduces overfitting (averaging the predictions of many trees), and can handle both categorical and numeric variables. Overall, it’s more reliable than a single classification tree.\n\n\n# install.packages(\"ranger\")\nlibrary(ranger)  # For random forest\n\nWarning: package 'ranger' was built under R version 4.3.3\n\n\n\nFit a random forest model with varying values for the mtry parameter\nThe fitting is based on 5 fold CV on the training data.\nInclude 6 predictors in the model\nset ‘trees’ = 100 and grid levels = 3 to reduce long execution time\n\n\nset.seed(123)\n# Take a 10% of the training data to improve the efficiency of model fitting\nrf_training_data &lt;- training_data %&gt;%\n  slice_sample(prop = 0.1)\n\n\n# Specify the random forest model\nrf_spec &lt;- rand_forest(\n  mtry = tune(),  # Number of predictors to randomly select at each split\n  trees = 100,    # Number of trees in the forest\n  min_n = tune()  # Minimum number of data points in a node\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\n# Create a recipe including 6 predictors\nrf_recipe &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + GenHlth + Age, data = rf_training_data)\n\n# Combine the model and recipe into a workflow\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(rf_recipe)\n\n# Define 5-fold cross-validation\nrf_cv_folds &lt;- vfold_cv(rf_training_data, v = 5, strata = Diabetes_binary)\n\n# Define a grid of `mtry` and `min_n` values\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 6)),  # Test mtry values between 2 and 6\n  min_n(range = c(2, 10)),  # Test min_n values between 2 and 10\n  levels = 3  # Use 3 levels for each parameter\n)\n\n# Tune the random forest model\nset.seed(123)\n\n# this step takes long time to execute\nrf_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = rf_cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(roc_auc)  # Use AUC-ROC as the evaluation metric\n)\n\n# Select the best parameters based on AUC-ROC\nrf_best_params &lt;- select_best(rf_results, metric = \"roc_auc\")\n\n# Finalize the workflow with the best hyperparameters\nrf_final_workflow &lt;- finalize_workflow(rf_workflow, rf_best_params)\n\n# Fit the final random forest model on the sampled training data\nrf_final_fit &lt;- fit(rf_final_workflow, data = rf_training_data)\n\n\n\n# Plot the results of the tuning\nautoplot(rf_results) + labs(title = \"Random Forest Tuning Results\")\n\n\n\n\n\n# Display the best parameters\nprint(rf_best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;               \n1     2     6 Preprocessor1_Model4\n\n\n\nFinal model selection. Now I have two best models from classification tree and random forest. I fit both to the test set and compare the fitting.\n\n\nlibrary(yardstick)\nlibrary(dplyr)\n# Select only the required columns from test_data\ntest_data &lt;- test_data %&gt;%\n  dplyr::select(Diabetes_binary, HighBP, HighChol, BMI, PhysActivity, GenHlth, Age)\n\n\nGenerate predictions for the two best models using test set\nGenerate the evaluations and save them in metrics\n\n\n# Generate predictions for the random forest model\nrf_predictions &lt;- predict(rf_final_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% dplyr::select(Diabetes_binary))\n\n# Generate predictions for the classification tree model\ntree_predictions &lt;- predict(final_tree_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% dplyr::select(Diabetes_binary))\n\n# Evaluate Random Forest on the test set\nrf_metrics &lt;- rf_predictions %&gt;%\n  roc_auc(truth = Diabetes_binary, .pred_Yes) %&gt;%\n  bind_rows(\n    accuracy(\n      data = rf_predictions %&gt;% \n        bind_cols(predict(rf_final_fit, test_data, type = \"class\") %&gt;% rename(.pred_class = .pred_class)),\n      truth = Diabetes_binary,\n      estimate = .pred_class\n    ),\n    mn_log_loss(rf_predictions, truth = Diabetes_binary, .pred_Yes)\n  ) %&gt;%\n  mutate(Model = \"Random Forest\")\n\n\n# the following is to evaluate the classification tree on the test set\n# Add class predictions explicitly for accuracy calculations\nclass_predictions &lt;- predict(final_tree_fit, test_data, type = \"class\") %&gt;%\n  pull(.pred_class)  # Extract as a vector\n\n# Evaluate the Classification Tree on the test set\ntree_metrics &lt;- tree_predictions %&gt;%\n  roc_auc(truth = Diabetes_binary, .pred_Yes) %&gt;%\n  bind_rows(\n    accuracy(\n      data = tree_predictions %&gt;% mutate(.pred_class = class_predictions),\n      truth = Diabetes_binary,\n      estimate = .pred_class\n    ),\n    mn_log_loss(tree_predictions, truth = Diabetes_binary, .pred_Yes)\n  ) %&gt;%\n  mutate(Model = \"Classification Tree\")\n\n\nComparison between the two best models.\nResults show the key metrics (roc_auc, accuracy, mn_log_loss), and based on that, the classification tree is overall marginally better. Both model have low roc_auc, indicating that when given an instance, the models has poor predictive power, can assign it to either class. It seems that both models have high accuracy which is a sign of imbalanced dataset. mn_log_loss is not low for either model, where the classification tree slightly outperform the random forest on this metric.\n\n\n# Combine and display results\ncomparison_metrics &lt;- bind_rows(rf_metrics, tree_metrics)\n\n# print(\"Comparison of Random Forest and Classification Tree Models on the Test Set:\")\ncomparison_metrics\n\n# A tibble: 6 × 4\n  .metric     .estimator .estimate Model              \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n1 roc_auc     binary         0.189 Random Forest      \n2 accuracy    binary         0.864 Random Forest      \n3 mn_log_loss binary         2.48  Random Forest      \n4 roc_auc     binary         0.205 Classification Tree\n5 accuracy    binary         0.860 Classification Tree\n6 mn_log_loss binary         2.34  Classification Tree"
  },
  {
    "objectID": "Modeling.html#basic-introduction",
    "href": "Modeling.html#basic-introduction",
    "title": "Modeling",
    "section": "",
    "text": "In this part, the goal is to create models for predicting the Diabetes_binary variable\nUse log-loss as the metric the evaluate the models\nFor both classification tree and random forest model type, use log-loss with 5 fold cross-validation to select the best model, respectively.\n\n\n\n\nA classification tree is a type of decision tree algorithm used for categorical response variables (in this example, Diabetes_binary)\nThe tree starts with a root node and then selects features that best splits the data into classes. At each node there will be a split, creating branches. Splitting stops when a criterion is met. Then at the leaf nodes, the class label is assigned based on the majority class of the training data in that node. To classify a new data point, the tree starts at the root node and follows the branches based on the feature values of the data point until it reaches a leaf node.\nNodes represent features; branches represent decision rules; leaves represent class labels (predicted outcomes)\nSplit the data as 70% being training set and 30% being test set\n\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(rsample)\nlibrary(dplyr)\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\nlibrary(tibble)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata1 &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata1 &lt;- data1 %&gt;%\n  mutate(\n    HighBP = factor(HighBP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HighChol = factor(HighChol, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Smoker = factor(Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Stroke = factor(Stroke, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    PhysActivity = factor(PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    GenHlth = factor(GenHlth, levels = c(1,2,3,4,5), labels = c(\"excellent\", \"very_good\", \"good\", \"fair\", \"poor\"))\n  )\ndata1 &lt;- data1 %&gt;%\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"No\", \"Yes\")))\n# Set a seed for reproducibility\nset.seed(123)\n\n# Split the data into training (70%) and test sets (30%)\n# resulting training set size: 177,575; resulting test set size: 76105\n\n# data_split &lt;- initial_split(data %&gt;% mutate(Diabetes_binary = factor(Diabetes_binary)), \n#                             prop = 0.7, \n#                             strata = Diabetes_binary)\n\n# # not working when rendering\n# data_split &lt;- initial_split(\n#   data = data %&gt;% dplyr::mutate(Diabetes_binary = as.factor(Diabetes_binary)), \n#   prop = 0.7, \n#   strata = Diabetes_binary\n# )\n\n# Get row indices for training set\n# train_indices &lt;- sample(seq_len(253680), size = floor(0.7 * 253680))\n# my_data &lt;- data \n# # Split the data\n# \n# # training_data &lt;- slice(data, train_indices)\n# # test_data &lt;- slice(data, -train_indices)\n# \n# training_data &lt;- my_data[train_indices, ]  # 70% of the data\n# test_data &lt;- my_data[-train_indices, ]        # Remaining 30% of the data\n# test_data\n\n\n# data_split &lt;- initial_split(data1, prop = 0.7, strata = Diabetes_binary)\n# training_data &lt;- training(data_split)\n# test_data &lt;- testing(data_split)\n# print(class(data1))\n\ntotal_rows &lt;- 253680\ntrain_indices &lt;- sample(seq_len(total_rows), size = floor(0.7 * total_rows))\n\n# Split using `data1`\ntraining_data &lt;- data1[train_indices, ]\ntest_data &lt;- data1[-train_indices, ]\ntest_data\n\n# A tibble: 76,104 × 22\n   Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;           &lt;fct&gt;  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt; \n 1 No              No     No               0    25 Yes    No    \n 2 No              Yes    No               1    27 No     No    \n 3 No              Yes    Yes              1    24 No     No    \n 4 No              Yes    Yes              1    25 Yes    No    \n 5 Yes             Yes    Yes              1    28 No     No    \n 6 No              No     Yes              1    33 Yes    Yes   \n 7 No              Yes    Yes              1    28 Yes    No    \n 8 No              No     No               1    32 No     No    \n 9 Yes             Yes    Yes              1    37 Yes    Yes   \n10 Yes             Yes    Yes              1    28 Yes    No    \n# ℹ 76,094 more rows\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;fct&gt;, PhysActivity &lt;fct&gt;,\n#   Fruits &lt;fct&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;fct&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;fct&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n# finally it worked. In Modeling.qmd, I need to do every step that I did in EDA.qmd. Environment of running quarto render is different from that in local PC \n\n\ncheck the sizes of training and test sets\n\n\ncat(\"Training set size:\", nrow(training_data), \"\\n\")\ncat(\"Test set size:\", nrow(test_data), \"\\n\")\n\n\nFit a classification tree with varying values for the complexity parameter\n\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(tidymodels)\n\ntraining_data &lt;- training_data %&gt;%\n  mutate(Diabetes_binary = as.factor(Diabetes_binary))\n\n# Set seed for reproducibility\nset.seed(123)\n# Choose a smaller dataset without compromising the results (otherwise the execution time is too long)\nsmall_training_data &lt;- training_data %&gt;%\n  slice_sample(prop = 0.4)\n\n# Specify the classification tree model with 6 predictor variables:  HighBP + HighChol + BMI + PhysActivity + HeartDiseaseorAttack + Age\ntree_recipe &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + GenHlth + Age, data = small_training_data)\n\n# Define the classification tree model with tuning parameters\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),  # Parameter to tune\n  tree_depth = tune(),       # Parameter to tune\n  min_n = tune()             # Parameter to tune\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"rpart\")\n\n\n# Create a workflow\ntree_workflow &lt;- workflow() %&gt;%\n  add_recipe(tree_recipe) %&gt;%\n  add_model(tree_spec)\n\n\n# Create 5-fold cross-validation folds on the small dataset\ncv_folds &lt;- vfold_cv(small_training_data, v = 5, strata = Diabetes_binary)\n\n# Set up a grid of hyperparameters to tune\ngrid &lt;- grid_regular(\n  cost_complexity(),\n  tree_depth(),\n  min_n(),\n  levels = 3\n)\n\n# Tune the model using log-loss. This step takes long time to execute\nset.seed(123)\n\ntree_results &lt;- tune_grid(\n  tree_workflow,\n  resamples = cv_folds,\n  grid = grid,\n  metrics = metric_set(mn_log_loss)  # Use log-loss as the evaluation metric\n)\n\n# Select the best based on log-loss\nbest_params &lt;- select_best(tree_results, metric = \"mn_log_loss\")\n\n# Finalize the workflow with the best parameters\nfinal_tree &lt;- finalize_workflow(tree_workflow, best_params)\n\n# Train the final model on the training set\nfinal_tree_fit &lt;- fit(final_tree, data = small_training_data)\n\n# Display the results\nautoplot(tree_results) + labs(title = \"Log-Loss for Classification Tree\")\n\n\n\n\n\n# Print the best parameters\nprint(best_params)\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         15    40 Preprocessor1_Model25\n\n\n\n# check the resulting model fit\ntree_results\nfinal_tree\nfinal_tree_fit\n\n\n\n\n\nA random forest is an ensemble machine learning algorithm that builds and combines multiple decision trees to improve accuracy and robustness.\nIt Creates many decision trees (often hundreds or thousands) from random subsets of the data (bootstrapping). At each split in a tree, only a random subset of predictors is considered. In our case, the previous step uses a classification tree model. And in this step, random forest uses majority voting where the class predicted most often by the trees is chosen.\nRandom forest improves over a Basic classification tree. It reduces overfitting (averaging the predictions of many trees), and can handle both categorical and numeric variables. Overall, it’s more reliable than a single classification tree.\n\n\n# install.packages(\"ranger\")\nlibrary(ranger)  # For random forest\n\nWarning: package 'ranger' was built under R version 4.3.3\n\n\n\nFit a random forest model with varying values for the mtry parameter\nThe fitting is based on 5 fold CV on the training data.\nInclude 6 predictors in the model\nset ‘trees’ = 100 and grid levels = 3 to reduce long execution time\n\n\nset.seed(123)\n# Take a 10% of the training data to improve the efficiency of model fitting\nrf_training_data &lt;- training_data %&gt;%\n  slice_sample(prop = 0.1)\n\n\n# Specify the random forest model\nrf_spec &lt;- rand_forest(\n  mtry = tune(),  # Number of predictors to randomly select at each split\n  trees = 100,    # Number of trees in the forest\n  min_n = tune()  # Minimum number of data points in a node\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\n# Create a recipe including 6 predictors\nrf_recipe &lt;- recipe(Diabetes_binary ~ HighBP + HighChol + BMI + PhysActivity + GenHlth + Age, data = rf_training_data)\n\n# Combine the model and recipe into a workflow\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(rf_recipe)\n\n# Define 5-fold cross-validation\nrf_cv_folds &lt;- vfold_cv(rf_training_data, v = 5, strata = Diabetes_binary)\n\n# Define a grid of `mtry` and `min_n` values\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 6)),  # Test mtry values between 2 and 6\n  min_n(range = c(2, 10)),  # Test min_n values between 2 and 10\n  levels = 3  # Use 3 levels for each parameter\n)\n\n# Tune the random forest model\nset.seed(123)\n\n# this step takes long time to execute\nrf_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = rf_cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(roc_auc)  # Use AUC-ROC as the evaluation metric\n)\n\n# Select the best parameters based on AUC-ROC\nrf_best_params &lt;- select_best(rf_results, metric = \"roc_auc\")\n\n# Finalize the workflow with the best hyperparameters\nrf_final_workflow &lt;- finalize_workflow(rf_workflow, rf_best_params)\n\n# Fit the final random forest model on the sampled training data\nrf_final_fit &lt;- fit(rf_final_workflow, data = rf_training_data)\n\n\n\n# Plot the results of the tuning\nautoplot(rf_results) + labs(title = \"Random Forest Tuning Results\")\n\n\n\n\n\n# Display the best parameters\nprint(rf_best_params)\n\n# A tibble: 1 × 3\n   mtry min_n .config             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;               \n1     2     6 Preprocessor1_Model4\n\n\n\nFinal model selection. Now I have two best models from classification tree and random forest. I fit both to the test set and compare the fitting.\n\n\nlibrary(yardstick)\nlibrary(dplyr)\n# Select only the required columns from test_data\ntest_data &lt;- test_data %&gt;%\n  dplyr::select(Diabetes_binary, HighBP, HighChol, BMI, PhysActivity, GenHlth, Age)\n\n\nGenerate predictions for the two best models using test set\nGenerate the evaluations and save them in metrics\n\n\n# Generate predictions for the random forest model\nrf_predictions &lt;- predict(rf_final_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% dplyr::select(Diabetes_binary))\n\n# Generate predictions for the classification tree model\ntree_predictions &lt;- predict(final_tree_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% dplyr::select(Diabetes_binary))\n\n# Evaluate Random Forest on the test set\nrf_metrics &lt;- rf_predictions %&gt;%\n  roc_auc(truth = Diabetes_binary, .pred_Yes) %&gt;%\n  bind_rows(\n    accuracy(\n      data = rf_predictions %&gt;% \n        bind_cols(predict(rf_final_fit, test_data, type = \"class\") %&gt;% rename(.pred_class = .pred_class)),\n      truth = Diabetes_binary,\n      estimate = .pred_class\n    ),\n    mn_log_loss(rf_predictions, truth = Diabetes_binary, .pred_Yes)\n  ) %&gt;%\n  mutate(Model = \"Random Forest\")\n\n\n# the following is to evaluate the classification tree on the test set\n# Add class predictions explicitly for accuracy calculations\nclass_predictions &lt;- predict(final_tree_fit, test_data, type = \"class\") %&gt;%\n  pull(.pred_class)  # Extract as a vector\n\n# Evaluate the Classification Tree on the test set\ntree_metrics &lt;- tree_predictions %&gt;%\n  roc_auc(truth = Diabetes_binary, .pred_Yes) %&gt;%\n  bind_rows(\n    accuracy(\n      data = tree_predictions %&gt;% mutate(.pred_class = class_predictions),\n      truth = Diabetes_binary,\n      estimate = .pred_class\n    ),\n    mn_log_loss(tree_predictions, truth = Diabetes_binary, .pred_Yes)\n  ) %&gt;%\n  mutate(Model = \"Classification Tree\")\n\n\nComparison between the two best models.\nResults show the key metrics (roc_auc, accuracy, mn_log_loss), and based on that, the classification tree is overall marginally better. Both model have low roc_auc, indicating that when given an instance, the models has poor predictive power, can assign it to either class. It seems that both models have high accuracy which is a sign of imbalanced dataset. mn_log_loss is not low for either model, where the classification tree slightly outperform the random forest on this metric.\n\n\n# Combine and display results\ncomparison_metrics &lt;- bind_rows(rf_metrics, tree_metrics)\n\n# print(\"Comparison of Random Forest and Classification Tree Models on the Test Set:\")\ncomparison_metrics\n\n# A tibble: 6 × 4\n  .metric     .estimator .estimate Model              \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;              \n1 roc_auc     binary         0.189 Random Forest      \n2 accuracy    binary         0.864 Random Forest      \n3 mn_log_loss binary         2.48  Random Forest      \n4 roc_auc     binary         0.205 Classification Tree\n5 accuracy    binary         0.860 Classification Tree\n6 mn_log_loss binary         2.34  Classification Tree"
  }
]